{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add to image_dataloader_config.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class DistortedNISTLoaderConfig:\n",
    "    name:str = \"DistortedNISTLoader\"\n",
    "    dataset_name:str = \"mnist\" # emnist, fashion, mnist\n",
    "    batch_size: int= 23\n",
    "    data_dir:str = image_data_path\n",
    "    \n",
    "    distortion: str = 'noise' # noise, swirl, pixelate, half_mask\n",
    "    distortion_level: float = 0.4  # 0.4, 5, 0.7, None\n",
    "    \n",
    "    max_node_num: int = None\n",
    "    max_feat_num: int = None\n",
    "\n",
    "    dimensions: int = None\n",
    "    vocab_size: int = 2\n",
    "    unet_resize: bool = False\n",
    "\n",
    "    pepper_threshold: float = 0.5\n",
    "    flatten: bool = True\n",
    "    as_image: bool = False\n",
    "\n",
    "    max_training_size:int = None\n",
    "    max_test_size:int = None\n",
    "\n",
    "    total_data_size: int = None\n",
    "    training_size: int = None\n",
    "    test_size: int = None\n",
    "    test_split: float = None\n",
    "\n",
    "    temporal_net_expected_shape : List[int] = None\n",
    "    data_min_max: List[float] = field(default_factory=lambda:[0.,1.])\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.dimensions, self.temporal_net_expected_shape = self.expected_shape(self.as_image,self.flatten,self.unet_resize)\n",
    "        self.number_of_nodes = self.max_node_num\n",
    "        self.number_of_labels = NUMBER_OF_LABELS[self.dataset_name]\n",
    "\n",
    "    def expected_shape(self,as_image,flatten,unet=False):\n",
    "        if as_image:\n",
    "            if flatten:\n",
    "                shape = [1,1,784]\n",
    "                dimensions = 784\n",
    "            else:\n",
    "                if unet:\n",
    "                    shape = [1, 32, 32]\n",
    "                    dimensions = 1024\n",
    "                else:\n",
    "                    shape = [1, 28, 28]\n",
    "                    dimensions = 784\n",
    "        else:\n",
    "            if flatten:\n",
    "                shape = [784]\n",
    "                dimensions = 784\n",
    "            else:\n",
    "                shape = [28,28]\n",
    "                dimensions = 784\n",
    "        return dimensions, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/df630/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/tqdm-4.66.1-py3.10.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Add this to 'image_dataloaders.py'\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from conditional_rate_matching.data.transforms import SqueezeTransform\n",
    "from conditional_rate_matching.data.transforms import FlattenTransform\n",
    "from conditional_rate_matching.data.image_dataloader_config import DistortedNISTLoaderConfig\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from skimage.transform import swirl\n",
    "\n",
    "\n",
    "def get_conditional_data(config: DistortedNISTLoaderConfig):\n",
    "    data_= config.dataset_name\n",
    "    threshold = config.pepper_threshold\n",
    "    dataloader_data_dir = config.data_dir\n",
    "    distortion = config.distortion\n",
    "    distortion_level = config.distortion_level\n",
    "\n",
    "    #...binerize MNIST images for dataset 1:\n",
    "\n",
    "    transformation_list = [transforms.ToTensor(), transforms.Lambda(lambda x: (x > threshold).float())]\n",
    "\n",
    "    #...define 1-parametric distortions for dataset 0:\n",
    "\n",
    "    distortion_list=[]\n",
    "    \n",
    "    if distortion == 'noise': \n",
    "        distortion_list.append(transforms.Lambda(lambda x: add_gaussian_noise(x, std=distortion_level)))\n",
    "        distortion_list.append(transforms.ToTensor())\n",
    "    \n",
    "    elif distortion == 'swirl': \n",
    "        distortion_list.append(transforms.Lambda(lambda x: apply_swirl(x, strength=distortion_level)))\n",
    "        distortion_list.append(transforms.ToTensor())\n",
    "\n",
    "    elif distortion == 'pixelate': \n",
    "        distortion_list.append(transforms.Lambda(lambda x: apply_coarse_grain(x, p=distortion_level)))\n",
    "        distortion_list.append(transforms.ToTensor())\n",
    "\n",
    "    elif distortion == 'half_mask':\n",
    "        distortion_list.append(transforms.Lambda(lambda x: apply_half_mask(x)))\n",
    "        distortion_list.append(transforms.ToTensor())\n",
    "\n",
    "    elif distortion == 'half_noise':\n",
    "        distortion_list.append(transforms.Lambda(lambda x: apply_half_noise(x, std=distortion_level)))\n",
    "        distortion_list.append(transforms.ToTensor())\n",
    "\n",
    "    distortion_list.append(transforms.Lambda(lambda x: (x > threshold).float()))\n",
    "\n",
    "    #...reshape images accordingly:\n",
    "\n",
    "    if config.flatten:\n",
    "        transformation_list.append(FlattenTransform)\n",
    "        distortion_list.append(FlattenTransform)\n",
    "\n",
    "    if not config.as_image:\n",
    "        transformation_list.append(SqueezeTransform)\n",
    "        distortion_list.append(SqueezeTransform)\n",
    "\n",
    "    if config.unet_resize:\n",
    "        transformation_list.append(transforms.Resize((32, 32)))\n",
    "        distortion_list.append(transforms.Resize((32, 32)))\n",
    "\n",
    "    #...compose relevant transformations:\n",
    "        \n",
    "    distort = transforms.Compose(distortion_list)\n",
    "    transform = transforms.Compose(transformation_list)\n",
    "\n",
    "    # Load MNIST dataset\n",
    "\n",
    "    if data_ == \"mnist\":\n",
    "        train_data_0 = datasets.MNIST(dataloader_data_dir, train=True, download=True, transform=distort)\n",
    "        test_data_0 = datasets.MNIST(dataloader_data_dir, train=False, download=True, transform=distort)\n",
    "        train_data_1 = datasets.MNIST(dataloader_data_dir, train=True, download=True, transform=transform)\n",
    "        test_data_1 = datasets.MNIST(dataloader_data_dir, train=False, download=True, transform=transform)\n",
    "    else:\n",
    "        raise Exception(\"Distortions only implemented for 'mnist' dataset!\")\n",
    "\n",
    "    return (train_data_0, test_data_0), (train_data_1, test_data_1)\n",
    "\n",
    "\n",
    "class DistortedNISTLoaderDataEdge:\n",
    "    def __init__(self, test_dl, train_dl):\n",
    "        self.test_dl = test_dl\n",
    "        self.train_dl = train_dl\n",
    "\n",
    "    def train(self):\n",
    "        return self.train_dl\n",
    "\n",
    "    def test(self):\n",
    "        return self.test_dl\n",
    "\n",
    "\n",
    "class CoupledNISTDataset(Dataset):\n",
    "    def __init__(self, dataset_0, dataset_1):\n",
    "        self.dataset_0 = dataset_0\n",
    "        self.dataset_1 = dataset_1\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset_0), len(self.dataset_1))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_0, _ = self.dataset_0[idx]\n",
    "        img_1, _ = self.dataset_1[idx]\n",
    "        return img_0, img_1 \n",
    "\n",
    "class DistortedNISTLoader:\n",
    "    config: DistortedNISTLoaderConfig\n",
    "    name: str = \"DistortedNISTDataloader\"\n",
    "\n",
    "    def __init__(self, config: DistortedNISTLoaderConfig):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        :param device:\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.number_of_spins = self.config.dimensions\n",
    "        data_0, data_1 = get_conditional_data(self.config)\n",
    "        self.create_conditional_dataloaders(data_1, data_0)\n",
    "\n",
    "    def train(self):\n",
    "        return self.train_conditional()\n",
    "\n",
    "    def test(self):\n",
    "        return self.test_conditional()\n",
    "\n",
    "    def train_conditional(self):\n",
    "        for databatch in self.data_train:\n",
    "            yield [databatch[0]],[databatch[1]]\n",
    "\n",
    "    def test_conditional(self):\n",
    "        for databatch in self.data_test:\n",
    "            yield [databatch[0]],[databatch[1]]\n",
    "\n",
    "    def define_sample_sizes(self):\n",
    "        self.training_data_size = self.config.training_size\n",
    "        self.test_data_size = self.config.test_size\n",
    "        self.total_data_size = self.training_data_size + self.test_data_size\n",
    "        self.config.training_proportion = float(self.training_data_size) / self.total_data_size\n",
    "\n",
    "    def create_conditional_dataloaders(self, data_1, data_0):\n",
    "        train_data_0, test_data_0 = data_0\n",
    "        train_data_1, test_data_1 = data_1\n",
    "\n",
    "        #=======================\n",
    "        # INDEPENDENT\n",
    "        #=======================\n",
    "\n",
    "        self.data0_train = DataLoader(train_data_0, batch_size=self.config.batch_size, shuffle=True)\n",
    "        self.data1_train = DataLoader(train_data_1, batch_size=self.config.batch_size, shuffle=True)\n",
    "        self.data0_test = DataLoader(test_data_0, batch_size=self.config.batch_size, shuffle=True)\n",
    "        self.data1_test = DataLoader(test_data_1, batch_size=self.config.batch_size, shuffle=True)\n",
    "        self.data1 = DistortedNISTLoaderDataEdge(self.data1_test, self.data1_train)\n",
    "        self.data0 = DistortedNISTLoaderDataEdge(self.data0_test, self.data0_train)\n",
    "\n",
    "        #=======================\n",
    "        # COUPLED\n",
    "        #=======================\n",
    "\n",
    "        train_ds = CoupledNISTDataset(train_data_0, train_data_1)\n",
    "        test_ds = CoupledNISTDataset(test_data_0, test_data_1)\n",
    "        self.data_train = DataLoader(train_ds, batch_size=self.config.batch_size, shuffle=True)\n",
    "        self.data_test = DataLoader(test_ds, batch_size=self.config.batch_size, shuffle=False)\n",
    "\n",
    "#...type of MNIST distortions\n",
    "\n",
    "def add_gaussian_noise(image, mean=0., std=0.4):\n",
    "    \"\"\"Adds Gaussian noise to a tensor.\"\"\"\n",
    "    image = torch.tensor(np.array(image, dtype='uint8'))\n",
    "    noise = torch.randn(image.size()) * std + mean\n",
    "    return image + noise\n",
    "\n",
    "def apply_swirl(image, strength=5, radius=20):\n",
    "    \"\"\"Apply swirl distortion to an image.\"\"\"\n",
    "    image_np = np.array(image)\n",
    "    swirled_image = swirl(image_np, strength=strength, radius=radius, mode='reflect')\n",
    "    return Image.fromarray(swirled_image)\n",
    "\n",
    "def apply_coarse_grain(image, p=0.7):\n",
    "    \"\"\"Coarse grains an image to a lower resolution.\"\"\"\n",
    "    old_size = image.size\n",
    "    if p <= 0: return image  \n",
    "    elif p >= 1: return Image.new('L', image.size, color=0)  # Return a black image\n",
    "    new_size = max(1, int(image.width * (1 - p))), max(1, int(image.height * (1 - p)))\n",
    "    image = image.resize(new_size, Image.BILINEAR)\n",
    "    return image.resize(old_size, Image.NEAREST) \n",
    "\n",
    "def apply_crop(image, R=14):\n",
    "    crop = transforms.Compose([transforms.CenterCrop(R), \n",
    "                               transforms.Pad(padding=int((28-R)/2), fill=0, padding_mode='constant')\n",
    "                              ])\n",
    "    return crop(image)\n",
    "\n",
    "def apply_half_mask(image):\n",
    "    \"\"\" Masks the first half of the image along its width. \"\"\"\n",
    "    mask_height = int(image.height / 2)\n",
    "    mask_width = image.width\n",
    "    mask_size = (mask_width, mask_height)\n",
    "    mask = Image.new('L', mask_size, color=255) \n",
    "    black_img = Image.new('L', image.size, color=0)\n",
    "    black_img.paste(mask, (0, 0))  \n",
    "    return Image.composite(image, black_img, black_img)\n",
    "\n",
    "def apply_half_noise(image, std=1):\n",
    "    \"\"\" Masks the first half of the image along its width. \"\"\"\n",
    "    mask_height = int(image.height / 2)\n",
    "    mask_width = image.width\n",
    "    mask_size = (mask_width, mask_height)\n",
    "    mask = Image.new('L', mask_size, color=255) \n",
    "    black_img = Image.new('L', image.size, color=0)\n",
    "    black_img.paste(mask, (0, 0))  \n",
    "    return Image.composite(image, black_img, black_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nGNgGLRA7ECbAozN78uKIif4+tdKuNydj6rIciJ7/06Gc7r/pqBodPv7VxTG1v63lhfFwpl/E+Byz//FoGhc/P8MN4yd8W8eqlMX/d0EdR9n89u/MFEWGMN714fpDAwM9g4WDGtQNTIYP/n799/fv3///vv797Yyms6zugYepa8XMjAsvshw7C4DDqD075woLjmGBX9dccqF/vtohFNy3r+lOOUYnn/BrTHj3wvcGi/8ncvAKwfnMqFJ/43e34xT57+/s2RxSNruaxBnw20rlQAAKNJLfTqR0FsAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from conditional_rate_matching.data.image_dataloader_config import NISTLoaderConfig\n",
    "\n",
    "config = NISTLoaderConfig(dataset_name=\"mnist\", batch_size=23)\n",
    "mnist = datasets.MNIST(config.data_dir, train=False, download=False)\n",
    "img_mnist = mnist[6][0]\n",
    "img_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897620/1430705484.py:3: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  pixelate = transforms.Resize((10, 10), interpolation=Image.NEAREST)\n"
     ]
    }
   ],
   "source": [
    "blur = transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0))\n",
    "center_crop = transforms.CenterCrop(10)\n",
    "pixelate = transforms.Resize((10, 10), interpolation=Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlklEQVR4nGNgGHZAZO/fyXBON5qk29+/ojC29j80yZl/E+Byz9El/5/hhjEz/s1Dk/y7iRXC4Gx++xcmyAJjeO/6MJ2BgcHewYJhDbprn/z9++/v379///39e1sZXaeugUfp64UMDIsvMhy7i64TBpT+nRPFJcew4K8rTrnQfx+NcErO+7cUpxzD8y+4NWb8e4Fb45AGAAq6NOYwZE9HAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "R=14\n",
    "\n",
    "# Define the crop and pad transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(R),  # Crop to 20x20\n",
    "    transforms.Pad(padding=int((28-R)/2), fill=0, padding_mode='constant')  # Pad back to 28x28 with black\n",
    "])\n",
    "\n",
    "transform(img_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABgklEQVR4nLXSyW7bMBAG4J+iqMWKrcWOGttJl0vf/5GCInZjV5a1m5IoTg+VGh8E9NQ5DfDhBzEzBP5LsakRgmPoBgIAgzEiDZij2b5vDUV2AwDLs6AaSRMKfx+LcigBwFw+LnSl5JQ0vKdvsT4aAMCczd5rKWVTUgT7fZAP3QAwe/3yLFKmCTAAwFztniPKTllH4Mvtbu30TYcRrXC3MfPDsVTg3tPLxsh+Xloa37SCODIvmrtkLuLvX900OVcDRuSut3TlZ3Yjsdp+8YrsdO0xoe7q2zLWkZRWtA3r4/up0X+xu77Z5LiRbLztJyO/nAuFD0xfVRPxoe25hep8SCQ+UOWqPIcOJyEqSn8ccnWH1PZV4lhMrN2H5vSWSLpDQHddCaxCQXXyXo7BP0uY+oc4tLtr2mIGreBxaTRZQ2wGnSB0TTB7MR3ZvEN75fs8D3zdzyQ5h2EK2xFsJqnKX8IpetI0gzJ5rZ3sUrd6BttkSHh9LeQ0J7tDJiwDavye/6zflze2gmPqKdEAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blur(img_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgElEQVR4nGNgGAUoQMaDj4GBQUSWESbAhJATKTf5x8DAm22GRZLFUWzxFwYWX+kj/zAl+cKufubksw5e/RqhHs769cxX+zG37fET/7BIfm0z0nguoLf0Cy73slf2cCFxmVAkxaz2f8cpacHxmB+npKBAhCwuKxnkA5RYcEoOXwAAaJYbyBBxeTMAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the crop and pad transformation pipeline\n",
    "R=14\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(R),  # Crop to 20x20\n",
    "    transforms.Pad(padding=int((28-R)/2), fill=0, padding_mode='constant')  # Pad back to 28x28 with black\n",
    "])\n",
    "transform(img_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1897620/2694848405.py:189: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  image = image.resize(new_size, Image.BILINEAR)\n",
      "/tmp/ipykernel_1897620/2694848405.py:190: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  return image.resize(old_size, Image.NEAREST)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4nL3NsSuEcRzH8ddxPaaLukFXTCxXjJIudlIWddmk/A3KbGPwJ4iVUf6A5w+w3HBhEIuO1Cnk6TmP4TnHgH63+EzfT+/f+/Pj/1PoXSWlljS/h3SkFLuobFt7L4dj1h1rMvDXbG5GVs3afM1LXcU9QWbVkiPNDMwv2/EYaNZMulFORKq2zsSyL3jh2a7rB5EZT6eS3AyYjW2oGb+SmnJy+QkDzDcNDQataDlM+jC7GbHm3N3PcNqEfe+9Hj47h1ELsU6/ZlvRooMs6M/C9zKs4tZL9tvj8HwA2GU0l01iEyoAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_coarse_grain(img_mnist, p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAADH0lEQVR4nAXBCUwTZhgG4Pf75fAAUaLZDLShFUSlxmlAaqQOqbCoqIhBBa+ZyOY6LdrEIwGxqAFBTawgHVEQRbwCIocca9lYZbgaxqrdUHBM1HBYD27REfr5PJBlkPqISao+4+uuHr9vidDlQN1771nEAv9JarHxhQckzktsX+mLwPxQWDVZ3CnXYZK9mEVD2XTkRY+iv86Ja9UG3A07Q5uu3idnh55EzOQgRJlPUu+qj+gcMJLRNIyXGf31z90NTmG7Y0HhBBu2SwzcZWpjpbqOVafnqhddiBUiu9HGOZUl7FjiCrl5Jvl7HKBIv3+xMKwPIjtgLvk376ZnyYkYOPYBURkn2GSLQ4BZy6JxfgUH/+5gz4lmKIsJQQW9CB/+D7XZReRSn/hkiuNdK3rS/8I1Q9Ohqp4XeDDR4bEvbRxC3lSLH83BFH9cR2PRDzHk147RxAHoA+pYqF5dR7vRzuGrx1HSHILLCZlU3eqFt8ETIIoC52FMcx3Neh8cOLyM0t9sY6+495jtFgiR361Acm0IXFd8hPuWCv7iSDmtDW/j5PQ5JGJ3DEKTO4xdQ/msVSSQ9Jd2VEdsoJD4vSyMKd6ojN1BN55soLxpDRy/WIrvDdF8N1AHcVAqo0jjkLOl6Sd+2aehU1+OIDU0i3KrZpE40VXP+ub5YoXLWUjPFbKl8QPsERpe6tnGYl3Le7w6/xv3T61BVqod5Q0mzuxKg7RERsLREUN7fP5HprUI+8VypPnHke/RJDrcoWKXGo8xdO/uxIVPI7DUlePQP61fm92Klbk+kSrxJuUruFUm0qLHSkxLsOHpD1auP/qULd5uEJurgrimyxW6ed7w/EaHUYWWRjZdQtmaKRCDFwuQ5FfDC74thfykFVsbTnHRrkzKedvCQrH+IX79eQaCy4DGR9Wouj0HKd0VvMrLCpEVY0GB04vSF++ksXPbMXmngjpvRtM2mTuEdq0Em/e8Zle5lq+q+6APSGJV6d8c6plA4s+wx9QWXwi/K7egpNnI6SmlvEcPMCP5DxbKyHJsHZdA4kiFS08u0N4KkkVh8DslfQZnolrh+2SyCAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width = 28, 28\n",
    "std = 1\n",
    "\n",
    "noise = std * torch.randn((height, width))\n",
    "noise = 255 * ((noise - noise.min()) / (noise.max() - noise.min())).numpy()\n",
    "img_noise = Image.fromarray(noise, mode='L')  \n",
    "img_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAADIElEQVR4nAXBDVDTdRgH8O/zixgMYrjwZC3lFDffuRRfgJVw/6UHIRpdIshbeXp0UJJQXLuuAMNretPOklMOTDItQWjtbi4QSxBF5ywRC46V3BCEcLDYLUbCydPng313rktVF49JW0Wwfhhz0vBnS/UDuy3S4hf/kMQn89PpQv5xunV4knuDIujI6hHMj7yL0uzdJJpfHuOMlzxYaWzFqahn8KB/ireFG6mo9wKLmOQZfBr8Hw4u3Iy2oH7+cmIZJaeEYOMiL8TgUBRmVTLEP/UjdjaT3p+o4DjTPKjlehL1PWOkiDnI/i/U1PTPSb7pqKGOd1p4XUo2i7AVX/H5AyUUcX8ZpqabkL6rG69bDuMvex2J6e+L6eGdS5wTdZTVe6op7oYS4drXEJ+yCmJu0sbvqYrIEKyDWozhhNWFj41BiJBdZuHoGsVTbyGXnIums881cFWVClJBLlnvJVDA55fsmg2BnS0md8mHzhAH9voV/HidD0+yyq8FaCrkWH96HDXKUARuj42pnNmvkVvXAt2R10RiWQeXe5VIzbSRPlpLMvswU9ldrOl0Q+QMbiCzYRRDTV9ze0YOO6VI/LZ6FrX3dCSKzM18+9VddE7hhn38AN1XtKI9NwBJx2+z6KhMRLl/CP+qr2JNjY1j/nyTDu1ZgAfbeyCObPkVr8h/5lKjHL6EXFSG2XiT43d+I1ADMZr7ED2rJtAVYaD9hhbc8vlgtIZTomMbiaTv3iZFgR9v6ZZAO/E8xpuTUJfv5zjvIIvq0lmQ3YW8xmdxNjqVRvpfoNS2X7DXE01CllzLPX3V9NHRFjYa2nhy0Id3demk2WRkMZPtRtGjSJ6qS6RFLi35i7W4aNZCtf4EicX5oQjVWmDuGEDeioVIa23kx8UNHHvmBxb140qsNbn4kPImB+/zoNtTST919YJC3BAZujza0S6JJ/FZ1FcxwNMNH3Bf4XKYdsxBrHRZWX96OR6NXOFvmxuR400hU6ZEnQkyiKQbHpg3Ozmt24YFG+upwGDhfKcb8+bCIP5GIaX9WAtn207QqTLeed6LtG8snFWmov8B5rZcGD5eayYAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width = 28, 28\n",
    "noise = std * torch.randn((height, width))\n",
    "noise = 255 * ((noise - noise.min()) / (noise.max() - noise.min())).numpy()\n",
    "mnist_noisy = np.array(img_mnist, dtype=np.float32) + noise\n",
    "img_noise = Image.fromarray(noise, mode='L')  \n",
    "img_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_single_image(image, figsize=(4, 4), cmap=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    img = image.permute(1, 2, 0)\n",
    "    if cmap is not None: ax.imshow(img, cmap=cmap)\n",
    "    else: ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_images(images, figsize=(4, 4), cmap=None):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = images[i].permute(1, 2, 0)\n",
    "        # img = img.squeeze()  # Squeeze the last dimension if it's 1\n",
    "        if cmap is not None: ax.imshow(img, cmap=cmap)\n",
    "        else: ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m data_config\u001b[39m.\u001b[39mdistortion_level \u001b[39m=\u001b[39m \u001b[39m0.4\u001b[39m\n\u001b[1;32m      6\u001b[0m dataloder \u001b[39m=\u001b[39m DistortedNISTLoader(data_config)\n\u001b[0;32m----> 7\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloder\u001b[39m.\u001b[39;49mdata_train\u001b[39m.\u001b[39;49m\u001b[39m__iter__\u001b[39;49m())\n\u001b[1;32m      8\u001b[0m plot_images(img)\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36mCoupledNISTDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> 107\u001b[0m     img_0, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_0[idx]\n\u001b[1;32m    108\u001b[0m     img_1, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_1[idx]\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m img_0, img_1\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.conda/envs/conditional_rate_matching/lib/python3.10/site-packages/torchvision/transforms/functional.py:140\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    138\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy(pic) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "from conditional_rate_matching.data.image_dataloader_config import DistortedNISTLoaderConfig\n",
    "\n",
    "data_config = DistortedNISTLoaderConfig(flatten=False, batch_size=1)\n",
    "data_config.distortion = 'noise'\n",
    "data_config.distortion_level = 0.4\n",
    "dataloder = DistortedNISTLoader(data_config)\n",
    "img = next(dataloder.data_train.__iter__())\n",
    "plot_images(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m dataloder \u001b[39m=\u001b[39m DistortedNISTLoader(data_config)\n\u001b[1;32m      5\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloder\u001b[39m.\u001b[39mdata_train\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m())\n\u001b[0;32m----> 6\u001b[0m plot_images(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_images' is not defined"
     ]
    }
   ],
   "source": [
    "data_config = DistortedNISTLoaderConfig(flatten=False, batch_size=1)\n",
    "data_config.distortion = 'swirl'\n",
    "data_config.distortion_level = -4\n",
    "dataloder = DistortedNISTLoader(data_config)\n",
    "img = next(dataloder.data_train.__iter__())\n",
    "plot_images(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1809047/2783328872.py:188: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  image = image.resize(new_size, Image.BILINEAR)\n",
      "/tmp/ipykernel_1809047/2783328872.py:189: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  return image.resize(old_size, Image.NEAREST)  # Resize back to 28x28\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAACgCAYAAABqgSVVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADLUlEQVR4nO3d0W3UQBRA0U2UKqiCJhAVUCUVIJqgCsrA/KNdXS+7a4/H53xHxCtGV0/z4uRtWZblAsBN73s/AMDohBIgCCVAEEqAIJQAQSgBglACBKEECB9rv/DL+7dXPgcT+vnn+6bfzxnlXmvPqIkSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAcLH3g/A//vx+9eqr/v66fNLnwPuccRza6IECEIJEIQSIAglQLDMAZ5i7ZLmkX9vrwWPiRIgCCVAEEqAIJQAwTJnQ8++7H729x3pTQjGsde5HYmJEiAIJUAQSoAglADBMgdOaoslzSMLwpGWSCZKgCCUAEEoAYJQAgTLnBtecZG815svI12K81oznduRmCgBglACBKEECEIJECxz4ABmX9KMvnA0UQIEoQQIQgkQhBIgTLPMefZl8EgX3Y+a6bOcwSNn+Qj/10f8G04mSoAglABBKAGCUAKEaZY5MLrZlzQzM1ECBKEECEIJEIQSIAy/zPHGzW1HfMPhLEb/tWFbmeWMmigBglACBKEECEIJEIQSIAy/9YbRnfEnM872OqaJEiAIJUAQSoAglABht2WOC/Db1n6WI3zm2ZxtiXG5nPMz/8tECRCEEiAIJUAQSoDgzRy4YZYlxit+N+ZIn28LJkqAIJQAQSgBglAChOGXOXtdGrsAZ62Zzug1zq2JEiAJJUAQSoAglABh+GUOzGKr5ctaljTrmSgBglACBKEECEIJEHZb5qy9SN7rb+u46ObaGbh2HrdY0jiP+zJRAgShBAhCCRCEEiB4MwfuYKlyTiZKgCCUAEEoAYJQAoThlzkuz4G9mSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAIJQAQSgBglACBKEECEIJEIQSILwty7Ls/RAAIzNRAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAKEv07Ce+yCjhvNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_config = DistortedNISTLoaderConfig(flatten=False, batch_size=1)\n",
    "data_config.distortion = 'pixelate'\n",
    "data_config.distortion_level = 0.5\n",
    "dataloder = DistortedNISTLoader(data_config)\n",
    "img = next(dataloder.data_train.__iter__())\n",
    "plot_images(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAACgCAYAAABqgSVVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADRElEQVR4nO3d0W3UQBRA0SSiCqqgCUQFVEkFiCaogjJwGnB0vbLXHk/O+Y60+zG5epone1+XZVleAPjQ29VfAGB0QgkQhBIgCCVAEEqAIJQAQSgBglAChC9b//D7289nfg8m9Of/r1M/zxnlUVvPqIkSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAEEqAsPk3c0b3+9/fSz73x9dvl3wu9+OM3peJEiAIJUAQSoAglADhlsucqy7F13z0XVygf27O6FxMlABBKAGCUAIEoQQIwy9z9lyKH31Z/ch3Wftbl+dzckbnZ6IECEIJEIQSIAglQBh+mbPVGZfQa58x0hMYjM0ZvS8TJUAQSoAglABBKAGCUAIEoQQIQgkQhBIgCCVAmObJnDM88oSD11VxBWf0OUyUAEEoAYJQAgShBAjDL3POeG2U11CxhzM6PxMlQBBKgCCUAEEoAcLwy5w1W58ocAHOVZzRuZgoAYJQAgShBAhCCRBuucy5g6Mv6b0Si6M5o9uZKAGCUAIEoQQIQgkQpl7mXHm5fMZrtma+PP8snNF7MFECBKEECEIJEIQSIEy9zLnSLJfYzOvoM7q2zNm6MBr9/8VECRCEEiAIJUAQSoAglADB1hs4xNrmeuvWe/THH02UAEEoAYJQAgShBAiWOcDT7FnwjMRECRCEEiAIJUAQSoBgmQOcauuCZ6SndUyUAEEoAYJQAgShBAiWOcCQvGYN4EaEEiAIJUAQSoBgmQM7bX1t2EjLCR5jogQIQgkQhBIgCCVAsMyBk4z02rAr+c0cgAkJJUAQSoAglADBMgd22vobMGv2LDauXAQdvZAZfallogQIQgkQhBIgCCVAsMyBJ9i6nNizFLnjEy4vL+MvbtaYKAGCUAIEoQQIQgkQLHPgQkcvNq5c8NxxSbOViRIgCCVAEEqAIJQAwTIHJjLzQuVKJkqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgCCUAEEoAYJQAgShBAhCCRCEEiAIJUAQSoAglABBKAHC67Isy9VfAmBkJkqAIJQAQSgBglACBKEECEIJEIQSIAglQBBKgPAOWSOS0ItLfHoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_config = DistortedNISTLoaderConfig(flatten=False, batch_size=1)\n",
    "data_config.distortion = 'half_mask'\n",
    "data_config.distortion_level = 1\n",
    "dataloder = DistortedNISTLoader(data_config)\n",
    "img = next(dataloder.data_train.__iter__())\n",
    "plot_images(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conditional_rate_matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
