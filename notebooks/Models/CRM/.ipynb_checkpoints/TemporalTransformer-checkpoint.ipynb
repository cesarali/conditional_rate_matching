{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90043154-d75b-4060-92dc-be7422b5eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "from pprint import pprint\n",
    "from conditional_rate_matching.data.music_dataloaders import LankhPianoRollDataloader\n",
    "from conditional_rate_matching.data.music_dataloaders_config import LakhPianoRollConfig\n",
    "\n",
    "\n",
    "from conditional_rate_matching.models.generative_models.crm import CRM\n",
    "from conditional_rate_matching.models.temporal_networks.temporal_networks_config import TemporalDeepMLPConfig\n",
    "from conditional_rate_matching.data.states_dataloaders_config import StatesDataloaderConfig\n",
    "from conditional_rate_matching.configs.configs_classes.config_crm import CRMConfig, CRMTrainerConfig, BasicPipelineConfig\n",
    "from conditional_rate_matching.models.temporal_networks.temporal_networks_config import DiffusersUnet2DConfig\n",
    "from conditional_rate_matching.data.music_dataloaders_config import LakhPianoRollConfig\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "config = CRMConfig()\n",
    "config.data0 = LakhPianoRollConfig(batch_size=batch_size,\n",
    "                                   conditional_model=True,\n",
    "                                   bridge_conditional=True)\n",
    "config.data1 = config.data0\n",
    "\n",
    "config.trainer = CRMTrainerConfig(\n",
    "    number_of_epochs=epochs,\n",
    "    learning_rate=1e-4,\n",
    "    metrics=[]\n",
    ")\n",
    "\n",
    "config.pipeline = BasicPipelineConfig(number_of_steps=5)\n",
    "config.temporal_network = TemporalDeepMLPConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64207649-7ec5-4138-8777-6d3cb4074786",
   "metadata": {},
   "outputs": [],
   "source": [
    "crm = CRM(config=config,device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75d1a5be-fd7a-43e8-bb8a-fbac5d2957fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int         # Vocabulary size\n",
    "    d_model: int            # Dimension of the model (embedding size)\n",
    "    nhead: int              # Number of attention heads\n",
    "    num_encoder_layers: int # Number of encoder layers\n",
    "    num_decoder_layers: int # Number of decoder layers\n",
    "    dim_feedforward: int    # Dimension of feedforward network\n",
    "    dropout: float          # Dropout rate\n",
    "    activation: str         # Activation function of the encoder/decoder intermediate layer\n",
    "    pad_token_id: int       # Padding token id for batch processing\n",
    "    time_embed_dim:int = 19\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "config = TransformerConfig(\n",
    "    vocab_size=crm.config.data1.vocab_size,        # replace with your vocab_size\n",
    "    d_model=crm.config.data1.dimensions, \n",
    "    nhead=2, \n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1,\n",
    "    activation='relu',\n",
    "    pad_token_id=0,          # assuming 0 is the pad token id\n",
    "    time_embed_dim=9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90cc79fb-2abf-4d44-a6a2-3da8981a75a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cesar\\anaconda4\\envs\\rate_matching\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "class MyTransformerWithConditioning(nn.Module):\n",
    "    def __init__(self, config:TransformerConfig):\n",
    "        super(MyTransformerWithConditioning, self).__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.continuous_embedding = nn.Linear(continuous_dim, config.d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, config.d_model))\n",
    "        self.continuous_dim = config.\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=config.d_model, \n",
    "            nhead=config.nhead, \n",
    "            num_encoder_layers=config.num_encoder_layers, \n",
    "            num_decoder_layers=config.num_decoder_layers, \n",
    "            dim_feedforward=config.dim_feedforward, \n",
    "            dropout=config.dropout,\n",
    "            activation=config.activation\n",
    "        )\n",
    "        self.out = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "    def forward(self, src, continuous_vector):\n",
    "        src_emb = self.embedding(src) + self.positional_encoding\n",
    "        continuous_emb = self.continuous_embedding(continuous_vector).unsqueeze(1)\n",
    "        combined_emb = torch.cat((continuous_emb, src_emb), dim=1)\n",
    "        output = self.transformer(combined_emb, combined_emb)\n",
    "        return self.out(output)\n",
    "\n",
    "# Example usage\n",
    "continuous_dim = 100  # dimension of your continuous vector\n",
    "model = MyTransformerWithConditioning(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "def2f4e7-d42e-42e2-9ba3-c2be50241ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conditional_rate_matching.models.temporal_networks.temporal_embedding_utils import transformer_timestep_embedding\n",
    "\n",
    "data_0,data_1 = next(crm.parent_dataloader.train().__iter__())\n",
    "times = torch.rand(data_0[0].size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d74c1503-86b9-43f9-a8cc-aee2cf68381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_embeddings = transformer_timestep_embedding(times, embedding_dim=config.time_embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84eee5e3-bbb0-44b5-b7ea-c9ac67e903d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 9])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d727023-3a8a-4004-ab29-ba3e237a0af3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyTransformerWithConditioning.forward() missing 1 required positional argument: 'continuous_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_0\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\rate_matching\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\rate_matching\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: MyTransformerWithConditioning.forward() missing 1 required positional argument: 'continuous_vector'"
     ]
    }
   ],
   "source": [
    "model(data_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
