{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d1bdab81-8a98-4261-a422-bb75dec83d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytest\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "from conditional_rate_matching.models.generative_models.crm import CRM\n",
    "from conditional_rate_matching.data.music_dataloaders import LankhPianoRollDataloader\n",
    "from conditional_rate_matching.data.music_dataloaders_config import LakhPianoRollConfig\n",
    "from conditional_rate_matching.models.temporal_networks.temporal_networks_config import SequenceTransformerConfig\n",
    "from conditional_rate_matching.models.temporal_networks.temporal_networks_utils import load_temporal_network\n",
    "from conditional_rate_matching.configs.experiments_configs.crm.crm_experiments_music import experiment_music_conditional_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b833f05a-4267-47db-8db6-476e8dd16c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n",
      "torch.Size([64, 256, 129])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:01,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "bridge_conditional = False\n",
    "config = experiment_music_conditional_config(bridge_conditional=bridge_conditional)\n",
    "config.temporal_network = SequenceTransformerConfig(num_layers=1,\n",
    "                                                    d_model=128,\n",
    "                                                    num_heads=1)\n",
    "\n",
    "crm = CRM(config)\n",
    "\n",
    "databatch = next(crm.dataloader_0.train().__iter__())\n",
    "x = databatch[0]\n",
    "batch_size = x.size(0)\n",
    "ts = torch.rand((batch_size,))\n",
    "print(x.shape)\n",
    "\n",
    "logits = crm.forward_rate.temporal_network(x,ts)\n",
    "print(logits.shape)\n",
    "\n",
    "generative_sample,original_sample = crm.pipeline(32,origin=True)\n",
    "print(generative_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8aa24a66-59de-4305-b961-30c9708ff31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_per_song(generated_song_notes,real_song_notes):\n",
    "    notes_in_generated_song = set(generated_song_notes.detach().numpy())\n",
    "    notes_in_real_song = set(real_song_notes.detach().numpy())\n",
    "    \n",
    "    notes_in_generated_not_in_real = notes_in_generated_song - notes_in_real_song\n",
    "    number_of_notes_not = len(notes_in_generated_not_in_real)\n",
    "    \n",
    "    total_proportion = number_of_notes_not/config.data1.vocab_size\n",
    "    \n",
    "    \n",
    "    tensor_notes_in_real = torch.Tensor(list(notes_in_real_song))\n",
    "    # Create a boolean mask for each element in tensor_a indicating whether it's in tensor_b\n",
    "    mask = torch.isin(generated_song_notes, tensor_notes_in_real)\n",
    "    \n",
    "    # Count the number of True values in the mask\n",
    "    count_of_notes_in_real = torch.sum(mask).item()\n",
    "    count_not_in_real = real_song_notes.size(0) - count_of_notes_in_real\n",
    "    \n",
    "    outliers_per_song = count_not_in_real/real_song_notes.size(0)\n",
    "    \n",
    "    return total_proportion,outliers_per_song\n",
    "    \n",
    "def obtain_numbers_not_in_real(a,b):\n",
    "    number_of_songs = generative_sample.size(0)\n",
    "\n",
    "    total_proportion_list  = []\n",
    "    outlier_per_song_list = []\n",
    "    \n",
    "    for song_index in range(number_of_songs):\n",
    "        generated_song_notes = generative_sample[song_index]\n",
    "        real_song_notes = original_sample[song_index]\n",
    "        proportion,outlier_ = outlier_per_song(generated_song_notes,real_song_notes)\n",
    "        total_proportion_list.append(proportion)\n",
    "        outlier_per_song_list.append(outlier_)\n",
    "    \n",
    "    total_proportion_list = np.asarray(total_proportion_list)\n",
    "    outlier_per_song_list = np.asarray(outlier_per_song_list)\n",
    "    \n",
    "    total_proportion_mean, total_proportionl_std = total_proportion_list.mean(),total_proportion_list.std()\n",
    "    outlier_per_song_mean, outlier_per_song_std = outlier_per_song_list.mean(),outlier_per_song_list.std()\n",
    "    return {total_proportion_mean, total_proportionl_std,outlier_per_song_mean, outlier_per_song_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "77ffc013-9af8-4d73-a375-33a9156df304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7080ba33-b7ca-4479-971f-aa94c10f05aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for note in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a1978022-d275-47f9-a57b-556937b25a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 5., 7.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(list(notes_in_real_song))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bad360d8-a110-43ed-8093-0a317a1d998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 7}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_in_generated_song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3831ee1b-1cda-4dbf-9096-5a865944458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in tensor_a that are present in tensor_b: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "tensor_a = torch.tensor([1, 2, 3, 4, 5,2,2,4])  # Tensor to check\n",
    "tensor_b = torch.tensor([2, 4, 6, 8])    # Tensor to check against\n",
    "\n",
    "# Flatten tensors if they are not already 1D (optional, depends on your case)\n",
    "tensor_a_flat = tensor_a.flatten()\n",
    "tensor_b_flat = tensor_b.flatten()\n",
    "\n",
    "# Create a boolean mask for each element in tensor_a indicating whether it's in tensor_b\n",
    "mask = torch.isin(tensor_a_flat, tensor_b_flat)\n",
    "\n",
    "# Count the number of True values in the mask\n",
    "count = torch.sum(mask).item()\n",
    "\n",
    "print(f\"Number of elements in tensor_a that are present in tensor_b: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30ad17-b4a9-4e9c-b8c3-70feb518420d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
